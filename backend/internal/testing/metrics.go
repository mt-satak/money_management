// ========================================
// ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ã¨ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ç›£è¦–
// ========================================

package testing

import (
	"context"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"log"
	"os"
	"path/filepath"
	"runtime"
	"sync"
	"time"

	"gorm.io/gorm"
)

// TestMetrics ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹
type TestMetrics struct {
	TestName     string            `json:"test_name"`
	StartTime    time.Time         `json:"start_time"`
	EndTime      time.Time         `json:"end_time"`
	Duration     time.Duration     `json:"duration"`
	Status       TestStatus        `json:"status"`
	DatabaseType string            `json:"database_type"` // "mysql", "sqlite", "inmemory"
	ParallelMode bool              `json:"parallel_mode"`
	MemoryUsage  MemoryMetrics     `json:"memory_usage"`
	DatabaseOps  DatabaseMetrics   `json:"database_ops"`
	Assertions   AssertionMetrics  `json:"assertions"`
	Tags         []string          `json:"tags"`
	ErrorMessage string            `json:"error_message,omitempty"`
	Metadata     map[string]string `json:"metadata,omitempty"`
}

// TestStatus ãƒ†ã‚¹ãƒˆå®Ÿè¡ŒçŠ¶æ…‹
type TestStatus string

const (
	StatusPassed  TestStatus = "passed"
	StatusFailed  TestStatus = "failed"
	StatusSkipped TestStatus = "skipped"
	StatusRunning TestStatus = "running"
)

// MemoryMetrics ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
type MemoryMetrics struct {
	HeapAlloc     uint64  `json:"heap_alloc"`      // ç¾åœ¨ã®ãƒ’ãƒ¼ãƒ—ä½¿ç”¨é‡
	HeapSys       uint64  `json:"heap_sys"`        // ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å–å¾—ã—ãŸãƒ’ãƒ¼ãƒ—ãƒ¡ãƒ¢ãƒª
	StackInuse    uint64  `json:"stack_inuse"`     // ã‚¹ã‚¿ãƒƒã‚¯ä½¿ç”¨é‡
	NumGC         uint32  `json:"num_gc"`          // GCå®Ÿè¡Œå›æ•°
	GCCPUFraction float64 `json:"gc_cpu_fraction"` // GCã«ã‚ˆã‚‹CPUä½¿ç”¨ç‡
}

// DatabaseMetrics ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãƒ¡ãƒˆãƒªã‚¯ã‚¹
type DatabaseMetrics struct {
	Connections    int           `json:"connections"`     // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šæ•°
	Queries        int           `json:"queries"`         // å®Ÿè¡Œã‚¯ã‚¨ãƒªæ•°
	Transactions   int           `json:"transactions"`    // ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•°
	CreatedRecords int           `json:"created_records"` // ä½œæˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
	UpdatedRecords int           `json:"updated_records"` // æ›´æ–°ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
	DeletedRecords int           `json:"deleted_records"` // å‰Šé™¤ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
	QueryTime      time.Duration `json:"query_time"`      // ç·ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚é–“
}

// AssertionMetrics ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹
type AssertionMetrics struct {
	Total  int `json:"total"`  // ç·ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°
	Passed int `json:"passed"` // æˆåŠŸã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°
	Failed int `json:"failed"` // å¤±æ•—ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°
}

// MetricsCollector ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å™¨
type MetricsCollector struct {
	mu        sync.RWMutex
	metrics   []TestMetrics
	outputDir string
	startTime time.Time
	memStats  runtime.MemStats
	dbTracker *DatabaseTracker
}

var (
	globalCollector *MetricsCollector
	collectorOnce   sync.Once
)

// GetMetricsCollector ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å™¨ã‚’å–å¾—
func GetMetricsCollector() *MetricsCollector {
	collectorOnce.Do(func() {
		outputDir := os.Getenv("TEST_METRICS_DIR")
		if outputDir == "" {
			outputDir = "./test-metrics"
		}

		globalCollector = &MetricsCollector{
			metrics:   make([]TestMetrics, 0),
			outputDir: outputDir,
			dbTracker: NewDatabaseTracker(),
		}

		// å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
		os.MkdirAll(outputDir, 0755)
	})
	return globalCollector
}

// StartTest ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
func (mc *MetricsCollector) StartTest(testName string, tags ...string) *TestSession {
	mc.mu.Lock()
	defer mc.mu.Unlock()

	// ãƒ¡ãƒ¢ãƒªçµ±è¨ˆã‚’å–å¾—
	runtime.ReadMemStats(&mc.memStats)

	session := &TestSession{
		collector: mc,
		metrics: TestMetrics{
			TestName:     testName,
			StartTime:    time.Now(),
			Status:       StatusRunning,
			Tags:         tags,
			DatabaseType: mc.detectDatabaseType(),
			ParallelMode: mc.isParallelMode(),
			MemoryUsage:  mc.collectMemoryMetrics(),
			Metadata:     make(map[string]string),
		},
	}

	config := GetGlobalConfig()
	if config.VerboseLogging {
		log.Printf("ğŸ“Š [%s] ãƒ†ã‚¹ãƒˆé–‹å§‹ - ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹", testName)
	}

	return session
}

// TestSession ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚»ãƒƒã‚·ãƒ§ãƒ³
type TestSession struct {
	collector *MetricsCollector
	metrics   TestMetrics
}

// AddAssertion ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³çµæœã‚’è¨˜éŒ²
func (ts *TestSession) AddAssertion(passed bool) {
	ts.metrics.Assertions.Total++
	if passed {
		ts.metrics.Assertions.Passed++
	} else {
		ts.metrics.Assertions.Failed++
	}
}

// AddDatabaseOp ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã‚’è¨˜éŒ²
func (ts *TestSession) AddDatabaseOp(opType string, recordCount int, duration time.Duration) {
	switch opType {
	case "create":
		ts.metrics.DatabaseOps.CreatedRecords += recordCount
	case "update":
		ts.metrics.DatabaseOps.UpdatedRecords += recordCount
	case "delete":
		ts.metrics.DatabaseOps.DeletedRecords += recordCount
	}
	ts.metrics.DatabaseOps.Queries++
	ts.metrics.DatabaseOps.QueryTime += duration
}

// SetMetadata ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
func (ts *TestSession) SetMetadata(key, value string) {
	if ts.metrics.Metadata == nil {
		ts.metrics.Metadata = make(map[string]string)
	}
	ts.metrics.Metadata[key] = value
}

// End ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
func (ts *TestSession) End(status TestStatus, errorMsg string) {
	ts.metrics.EndTime = time.Now()
	ts.metrics.Duration = ts.metrics.EndTime.Sub(ts.metrics.StartTime)
	ts.metrics.Status = status
	ts.metrics.ErrorMessage = errorMsg

	// æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
	ts.metrics.MemoryUsage = ts.collector.collectMemoryMetrics()

	ts.collector.mu.Lock()
	defer ts.collector.mu.Unlock()

	ts.collector.metrics = append(ts.collector.metrics, ts.metrics)

	config := GetGlobalConfig()
	if config.VerboseLogging {
		log.Printf("ğŸ“Š [%s] ãƒ†ã‚¹ãƒˆå®Œäº† - %s (å®Ÿè¡Œæ™‚é–“: %v)",
			ts.metrics.TestName, status, ts.metrics.Duration)
	}
}

// collectMemoryMetrics ãƒ¡ãƒ¢ãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
func (mc *MetricsCollector) collectMemoryMetrics() MemoryMetrics {
	var ms runtime.MemStats
	runtime.ReadMemStats(&ms)

	return MemoryMetrics{
		HeapAlloc:     ms.HeapAlloc,
		HeapSys:       ms.HeapSys,
		StackInuse:    ms.StackInuse,
		NumGC:         ms.NumGC,
		GCCPUFraction: ms.GCCPUFraction,
	}
}

// detectDatabaseType ä½¿ç”¨ä¸­ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º
func (mc *MetricsCollector) detectDatabaseType() string {
	config := GetGlobalConfig()
	if config.UseInMemoryDB {
		return "inmemory"
	} else if config.ParallelTestEnabled {
		return "mysql-parallel"
	}
	return "mysql"
}

// isParallelMode ä¸¦åˆ—å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
func (mc *MetricsCollector) isParallelMode() bool {
	config := GetGlobalConfig()
	return config.ParallelTestEnabled || config.UseInMemoryDB
}

// ExportMetrics ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
func (mc *MetricsCollector) ExportMetrics(format string) error {
	mc.mu.RLock()
	defer mc.mu.RUnlock()

	timestamp := time.Now().Format("20060102_150405")

	switch format {
	case "json":
		return mc.exportJSON(timestamp)
	case "csv":
		return mc.exportCSV(timestamp)
	case "summary":
		return mc.exportSummary(timestamp)
	default:
		return fmt.Errorf("unsupported format: %s", format)
	}
}

// exportJSON JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
func (mc *MetricsCollector) exportJSON(timestamp string) error {
	filename := filepath.Join(mc.outputDir, fmt.Sprintf("test-metrics_%s.json", timestamp))

	data, err := json.MarshalIndent(mc.metrics, "", "  ")
	if err != nil {
		return fmt.Errorf("JSON marshal failed: %v", err)
	}

	return ioutil.WriteFile(filename, data, 0644)
}

// exportCSV CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
func (mc *MetricsCollector) exportCSV(timestamp string) error {
	filename := filepath.Join(mc.outputDir, fmt.Sprintf("test-metrics_%s.csv", timestamp))

	content := "test_name,duration_ms,status,database_type,parallel_mode,memory_heap_mb,queries,assertions_total,assertions_failed\n"

	for _, metric := range mc.metrics {
		content += fmt.Sprintf("%s,%d,%s,%s,%t,%.2f,%d,%d,%d\n",
			metric.TestName,
			metric.Duration.Milliseconds(),
			metric.Status,
			metric.DatabaseType,
			metric.ParallelMode,
			float64(metric.MemoryUsage.HeapAlloc)/1024/1024,
			metric.DatabaseOps.Queries,
			metric.Assertions.Total,
			metric.Assertions.Failed,
		)
	}

	return ioutil.WriteFile(filename, []byte(content), 0644)
}

// exportSummary ã‚µãƒãƒªãƒ¼å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
func (mc *MetricsCollector) exportSummary(timestamp string) error {
	filename := filepath.Join(mc.outputDir, fmt.Sprintf("test-summary_%s.txt", timestamp))

	summary := mc.GenerateSummary()
	return ioutil.WriteFile(filename, []byte(summary), 0644)
}

// GenerateSummary å®Ÿè¡Œã‚µãƒãƒªãƒ¼ç”Ÿæˆ
func (mc *MetricsCollector) GenerateSummary() string {
	mc.mu.RLock()
	defer mc.mu.RUnlock()

	if len(mc.metrics) == 0 {
		return "ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹: ãƒ‡ãƒ¼ã‚¿ãªã—\n"
	}

	var (
		totalTests      = len(mc.metrics)
		passedTests     = 0
		failedTests     = 0
		skippedTests    = 0
		totalDuration   time.Duration
		totalQueries    = 0
		totalAssertion  = 0
		failedAssertion = 0
		maxMemory       uint64
		dbTypes         = make(map[string]int)
	)

	for _, m := range mc.metrics {
		switch m.Status {
		case StatusPassed:
			passedTests++
		case StatusFailed:
			failedTests++
		case StatusSkipped:
			skippedTests++
		}

		totalDuration += m.Duration
		totalQueries += m.DatabaseOps.Queries
		totalAssertion += m.Assertions.Total
		failedAssertion += m.Assertions.Failed

		if m.MemoryUsage.HeapAlloc > maxMemory {
			maxMemory = m.MemoryUsage.HeapAlloc
		}

		dbTypes[m.DatabaseType]++
	}

	successRate := float64(passedTests) / float64(totalTests) * 100
	avgDuration := totalDuration / time.Duration(totalTests)

	summary := fmt.Sprintf(`ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚µãƒãƒªãƒ¼
======================================

ğŸ”¢ å®Ÿè¡Œçµ±è¨ˆ:
   ç·ãƒ†ã‚¹ãƒˆæ•°:     %d
   æˆåŠŸ:           %d (%.1f%%)
   å¤±æ•—:           %d
   ã‚¹ã‚­ãƒƒãƒ—:       %d

â±ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:
   ç·å®Ÿè¡Œæ™‚é–“:     %v
   å¹³å‡å®Ÿè¡Œæ™‚é–“:   %v
   ç·ã‚¯ã‚¨ãƒªæ•°:     %d
   æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨: %.2f MB

âœ… ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³:
   ç·ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³: %d
   å¤±æ•—ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³: %d
   ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æˆåŠŸç‡: %.1f%%

ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ©ç”¨:
`,
		totalTests, passedTests, successRate, failedTests, skippedTests,
		totalDuration, avgDuration, totalQueries,
		float64(maxMemory)/1024/1024,
		totalAssertion, failedAssertion,
		float64(totalAssertion-failedAssertion)/float64(totalAssertion)*100,
	)

	for dbType, count := range dbTypes {
		summary += fmt.Sprintf("   %s: %d ãƒ†ã‚¹ãƒˆ\n", dbType, count)
	}

	summary += fmt.Sprintf("\nğŸ“… ç”Ÿæˆæ—¥æ™‚: %s\n", time.Now().Format("2006-01-02 15:04:05"))

	return summary
}

// DatabaseTracker ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œè¿½è·¡
type DatabaseTracker struct {
	mu          sync.RWMutex
	connections map[*gorm.DB]*DBMetrics
}

// DBMetrics ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
type DBMetrics struct {
	QueryCount       int
	TransactionCount int
	LastActivity     time.Time
}

// NewDatabaseTracker ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¿½è·¡å™¨ä½œæˆ
func NewDatabaseTracker() *DatabaseTracker {
	return &DatabaseTracker{
		connections: make(map[*gorm.DB]*DBMetrics),
	}
}

// TrackDB ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’è¿½è·¡
func (dt *DatabaseTracker) TrackDB(db *gorm.DB) {
	dt.mu.Lock()
	defer dt.mu.Unlock()

	dt.connections[db] = &DBMetrics{
		LastActivity: time.Now(),
	}
}

// RecordQuery ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚’è¨˜éŒ²
func (dt *DatabaseTracker) RecordQuery(db *gorm.DB) {
	dt.mu.Lock()
	defer dt.mu.Unlock()

	if metrics, exists := dt.connections[db]; exists {
		metrics.QueryCount++
		metrics.LastActivity = time.Now()
	}
}

// GetActiveConnections ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æ¥ç¶šæ•°ã‚’å–å¾—
func (dt *DatabaseTracker) GetActiveConnections() int {
	dt.mu.RLock()
	defer dt.mu.RUnlock()

	return len(dt.connections)
}

// CleanupExpiredConnections æœŸé™åˆ‡ã‚Œæ¥ç¶šã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
func (dt *DatabaseTracker) CleanupExpiredConnections(ctx context.Context, maxAge time.Duration) {
	ticker := time.NewTicker(time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			dt.mu.Lock()
			now := time.Now()
			for db, metrics := range dt.connections {
				if now.Sub(metrics.LastActivity) > maxAge {
					delete(dt.connections, db)
				}
			}
			dt.mu.Unlock()
		}
	}
}
