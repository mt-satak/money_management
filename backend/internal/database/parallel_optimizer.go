// ========================================
// ä¸¦åˆ—å®Ÿè¡Œåº¦è‡ªå‹•èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ 
// CPUãƒ»ãƒ¡ãƒ¢ãƒªãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã«ã‚ˆã‚‹å‹•çš„ä¸¦åˆ—åº¦åˆ¶å¾¡
// ========================================

package database

import (
	"context"
	"log"
	"runtime"
	"sync"
	"sync/atomic"
	"time"
)

// ParallelOptimizer ä¸¦åˆ—å®Ÿè¡Œåº¦æœ€é©åŒ–å™¨
type ParallelOptimizer struct {
	mu                  sync.RWMutex
	config              *ParallelConfig
	metrics             *ParallelMetrics
	resourceMonitor     *SystemResourceMonitor
	testTracker         *ActiveTestTracker
	optimizationHistory []*OptimizationEvent
}

// ParallelConfig ä¸¦åˆ—å®Ÿè¡Œè¨­å®š
type ParallelConfig struct {
	// åŸºæœ¬è¨­å®š
	MinParallelism     int `json:"min_parallelism"`     // æœ€å°ä¸¦åˆ—æ•°
	MaxParallelism     int `json:"max_parallelism"`     // æœ€å¤§ä¸¦åˆ—æ•°
	CurrentParallelism int `json:"current_parallelism"` // ç¾åœ¨ã®ä¸¦åˆ—æ•°
	DefaultParallelism int `json:"default_parallelism"` // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸¦åˆ—æ•°

	// ãƒªã‚½ãƒ¼ã‚¹é–¾å€¤
	CPUThresholdLow     float64 `json:"cpu_threshold_low"`     // CPUä½è² è·é–¾å€¤ (ä¸¦åˆ—å¢—)
	CPUThresholdHigh    float64 `json:"cpu_threshold_high"`    // CPUé«˜è² è·é–¾å€¤ (ä¸¦åˆ—æ¸›)
	MemoryThresholdLow  float64 `json:"memory_threshold_low"`  // ãƒ¡ãƒ¢ãƒªä½ä½¿ç”¨é–¾å€¤
	MemoryThresholdHigh float64 `json:"memory_threshold_high"` // ãƒ¡ãƒ¢ãƒªé«˜ä½¿ç”¨é–¾å€¤

	// èª¿æ•´è¨­å®š
	AdjustmentStep     int           `json:"adjustment_step"`     // èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—æ•°
	MonitoringInterval time.Duration `json:"monitoring_interval"` // ç›£è¦–é–“éš”
	AdaptiveMode       bool          `json:"adaptive_mode"`       // é©å¿œãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹
	StabilityWindow    int           `json:"stability_window"`    // å®‰å®šæ€§åˆ¤å®šã‚¦ã‚£ãƒ³ãƒ‰ã‚¦

	// ç’°å¢ƒåˆ¥è¨­å®š
	Environment  string `json:"environment"`   // å®Ÿè¡Œç’°å¢ƒ
	AutoOptimize bool   `json:"auto_optimize"` // è‡ªå‹•æœ€é©åŒ–æœ‰åŠ¹
}

// ParallelMetrics ä¸¦åˆ—å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹
type ParallelMetrics struct {
	ActiveTests         int32         `json:"active_tests"`          // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆæ•°
	CompletedTests      int32         `json:"completed_tests"`       // å®Œäº†ãƒ†ã‚¹ãƒˆæ•°
	FailedTests         int32         `json:"failed_tests"`          // å¤±æ•—ãƒ†ã‚¹ãƒˆæ•°
	AverageTestDuration time.Duration `json:"avg_test_duration"`     // å¹³å‡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“
	TotalTestDuration   time.Duration `json:"total_test_duration"`   // ç·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“
	ThroughputPerSecond float64       `json:"throughput_per_second"` // ç§’ã‚ãŸã‚Šãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ•°

	// ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
	PeakMemoryUsage uint64  `json:"peak_memory_usage"` // ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
	AverageCPUUsage float64 `json:"average_cpu_usage"` // å¹³å‡CPUä½¿ç”¨ç‡
	GCPressure      int     `json:"gc_pressure"`       // GCåœ§è¿«åº¦
}

// SystemResourceMonitor ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
type SystemResourceMonitor struct {
	CPUUsage        float64   `json:"cpu_usage"`
	MemoryUsage     float64   `json:"memory_usage"`
	AvailableMemory uint64    `json:"available_memory"`
	GoroutineCount  int       `json:"goroutine_count"`
	GCFrequency     uint32    `json:"gc_frequency"`
	LoadAverage     float64   `json:"load_average"`
	LastUpdate      time.Time `json:"last_update"`
}

// ActiveTestTracker ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆè¿½è·¡
type ActiveTestTracker struct {
	mu            sync.RWMutex
	activeTests   map[string]*TestExecution
	testStartTime map[string]time.Time
}

// TestExecution ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæƒ…å ±
type TestExecution struct {
	TestName         string        `json:"test_name"`
	StartTime        time.Time     `json:"start_time"`
	ExpectedDuration time.Duration `json:"expected_duration"`
	ResourceWeight   float64       `json:"resource_weight"` // ãƒªã‚½ãƒ¼ã‚¹é‡ã¿(1.0=æ¨™æº–)
	Priority         int           `json:"priority"`        // å„ªå…ˆåº¦(1-10)
}

// OptimizationEvent æœ€é©åŒ–ã‚¤ãƒ™ãƒ³ãƒˆ
type OptimizationEvent struct {
	Timestamp      time.Time `json:"timestamp"`
	OldParallelism int       `json:"old_parallelism"`
	NewParallelism int       `json:"new_parallelism"`
	Reason         string    `json:"reason"`
	CPUUsage       float64   `json:"cpu_usage"`
	MemoryUsage    float64   `json:"memory_usage"`
	ActiveTests    int       `json:"active_tests"`
	Effectiveness  float64   `json:"effectiveness"` // æœ€é©åŒ–åŠ¹æœ
}

var (
	globalParallelOptimizer *ParallelOptimizer
	parallelOptimizerOnce   sync.Once
)

// GetGlobalParallelOptimizer ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¸¦åˆ—æœ€é©åŒ–å™¨å–å¾—
func GetGlobalParallelOptimizer() *ParallelOptimizer {
	parallelOptimizerOnce.Do(func() {
		globalParallelOptimizer = &ParallelOptimizer{
			config:              getDefaultParallelConfig(),
			metrics:             &ParallelMetrics{},
			resourceMonitor:     &SystemResourceMonitor{},
			testTracker:         newActiveTestTracker(),
			optimizationHistory: make([]*OptimizationEvent, 0),
		}
	})
	return globalParallelOptimizer
}

// getDefaultParallelConfig ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¸¦åˆ—è¨­å®šå–å¾—
func getDefaultParallelConfig() *ParallelConfig {
	numCPU := runtime.NumCPU()

	return &ParallelConfig{
		MinParallelism:      1,
		MaxParallelism:      numCPU * 4, // CPUæ•°ã®4å€ã¾ã§
		CurrentParallelism:  numCPU,     // CPUæ•°ã¨åŒã˜
		DefaultParallelism:  numCPU,
		CPUThresholdLow:     0.3,                 // 30%ä»¥ä¸‹ã§ä¸¦åˆ—åº¦å¢—åŠ 
		CPUThresholdHigh:    0.8,                 // 80%ä»¥ä¸Šã§ä¸¦åˆ—åº¦å‰Šæ¸›
		MemoryThresholdLow:  0.4,                 // 40%ä»¥ä¸‹
		MemoryThresholdHigh: 0.85,                // 85%ä»¥ä¸Š
		AdjustmentStep:      maxInt(1, numCPU/4), // èª¿æ•´ã‚¹ãƒ†ãƒƒãƒ—
		MonitoringInterval:  5 * time.Second,
		AdaptiveMode:        true,
		StabilityWindow:     3, // 3å›é€£ç¶šã§å®‰å®šåˆ¤å®š
		Environment:         "testing",
		AutoOptimize:        true,
	}
}

// newActiveTestTracker ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆè¿½è·¡å™¨ä½œæˆ
func newActiveTestTracker() *ActiveTestTracker {
	return &ActiveTestTracker{
		activeTests:   make(map[string]*TestExecution),
		testStartTime: make(map[string]time.Time),
	}
}

// OptimizeParallelism ä¸¦åˆ—åº¦æœ€é©åŒ–å®Ÿè¡Œ
func (po *ParallelOptimizer) OptimizeParallelism() *OptimizationEvent {
	po.mu.Lock()
	defer po.mu.Unlock()

	// ç¾åœ¨ã®çŠ¶æ³åˆ†æ
	po.updateResourceMonitor()
	po.updateMetrics()

	// æœ€é©åŒ–åˆ¤å®š
	event := po.calculateOptimalParallelism()

	// æœ€é©åŒ–é©ç”¨
	if event.NewParallelism != event.OldParallelism {
		po.applyParallelismChange(event)
		po.recordOptimizationEvent(event)

		// ãƒ­ã‚°å‡ºåŠ›ï¼ˆå¸¸ã«æœ‰åŠ¹ï¼‰
		po.logOptimizationEvent(event)
	}

	return event
}

// updateResourceMonitor ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–æ›´æ–°
func (po *ParallelOptimizer) updateResourceMonitor() {
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)

	po.resourceMonitor = &SystemResourceMonitor{
		CPUUsage:        po.estimateCPULoad(),
		MemoryUsage:     float64(memStats.Alloc) / float64(memStats.Sys),
		AvailableMemory: memStats.Sys - memStats.Alloc,
		GoroutineCount:  runtime.NumGoroutine(),
		GCFrequency:     memStats.NumGC,
		LoadAverage:     po.calculateLoadAverage(),
		LastUpdate:      time.Now(),
	}
}

// estimateCPULoad CPUè² è·æ¨å®š
func (po *ParallelOptimizer) estimateCPULoad() float64 {
	// ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆæ•°ã¨ã‚´ãƒ«ãƒ¼ãƒãƒ³æ•°ã‹ã‚‰CPUè² è·ã‚’æ¨å®š
	activeTests := atomic.LoadInt32(&po.metrics.ActiveTests)
	goroutines := runtime.NumGoroutine()

	// åŸºæº–å€¤ã¨ã®æ¯”ç‡ã§CPUä½¿ç”¨ç‡ã‚’æ¨å®š
	baseline := float64(runtime.NumCPU() * 10) // CPUæ•°Ã—10ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
	currentLoad := float64(goroutines) / baseline

	// ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆã®å½±éŸ¿ã‚’åŠ å‘³
	testLoad := float64(activeTests) / float64(po.config.CurrentParallelism)

	estimatedCPU := (currentLoad + testLoad) / 2
	if estimatedCPU > 1.0 {
		estimatedCPU = 1.0
	}

	return estimatedCPU
}

// calculateLoadAverage è² è·å¹³å‡è¨ˆç®—
func (po *ParallelOptimizer) calculateLoadAverage() float64 {
	activeTests := atomic.LoadInt32(&po.metrics.ActiveTests)
	maxParallel := float64(po.config.CurrentParallelism)

	if maxParallel == 0 {
		return 0
	}

	return float64(activeTests) / maxParallel
}

// updateMetrics ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
func (po *ParallelOptimizer) updateMetrics() {
	po.testTracker.mu.RLock()
	activeCount := len(po.testTracker.activeTests)
	po.testTracker.mu.RUnlock()

	atomic.StoreInt32(&po.metrics.ActiveTests, int32(activeCount))

	// å¹³å‡å®Ÿè¡Œæ™‚é–“è¨ˆç®—
	po.calculateAverageTestDuration()

	// ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
	po.calculateThroughput()
}

// calculateAverageTestDuration å¹³å‡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“è¨ˆç®—
func (po *ParallelOptimizer) calculateAverageTestDuration() {
	po.testTracker.mu.RLock()
	defer po.testTracker.mu.RUnlock()

	if len(po.testTracker.testStartTime) == 0 {
		return
	}

	var totalDuration time.Duration
	var testCount int

	now := time.Now()
	for testName, startTime := range po.testTracker.testStartTime {
		if _, active := po.testTracker.activeTests[testName]; active {
			duration := now.Sub(startTime)
			totalDuration += duration
			testCount++
		}
	}

	if testCount > 0 {
		po.metrics.AverageTestDuration = totalDuration / time.Duration(testCount)
	}
}

// calculateThroughput ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
func (po *ParallelOptimizer) calculateThroughput() {
	completedTests := atomic.LoadInt32(&po.metrics.CompletedTests)
	totalDuration := po.metrics.TotalTestDuration

	if totalDuration > 0 {
		po.metrics.ThroughputPerSecond = float64(completedTests) / totalDuration.Seconds()
	}
}

// calculateOptimalParallelism æœ€é©ä¸¦åˆ—åº¦è¨ˆç®—
func (po *ParallelOptimizer) calculateOptimalParallelism() *OptimizationEvent {
	current := po.config.CurrentParallelism
	optimal := current
	reason := "ç¾çŠ¶ç¶­æŒ"

	cpuUsage := po.resourceMonitor.CPUUsage
	memoryUsage := po.resourceMonitor.MemoryUsage
	activeTests := atomic.LoadInt32(&po.metrics.ActiveTests)

	// æœ€é©åŒ–ãƒ­ã‚¸ãƒƒã‚¯
	if cpuUsage < po.config.CPUThresholdLow && memoryUsage < po.config.MemoryThresholdLow {
		// ãƒªã‚½ãƒ¼ã‚¹ä½™è£•ã‚ã‚Š â†’ ä¸¦åˆ—åº¦å¢—åŠ 
		if current < po.config.MaxParallelism {
			optimal = minInt(current+po.config.AdjustmentStep, po.config.MaxParallelism)
			reason = "ãƒªã‚½ãƒ¼ã‚¹ä½™è£•ã«ã‚ˆã‚Šä¸¦åˆ—åº¦å¢—åŠ "
		}

	} else if cpuUsage > po.config.CPUThresholdHigh || memoryUsage > po.config.MemoryThresholdHigh {
		// ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ â†’ ä¸¦åˆ—åº¦å‰Šæ¸›
		if current > po.config.MinParallelism {
			optimal = maxInt(current-po.config.AdjustmentStep, po.config.MinParallelism)
			reason = "ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã«ã‚ˆã‚Šä¸¦åˆ—åº¦å‰Šæ¸›"
		}

	} else if int(activeTests) < current/2 {
		// ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆæ•°ãŒå°‘ãªã„ â†’ ä¸¦åˆ—åº¦å‰Šæ¸›
		if current > po.config.MinParallelism {
			optimal = maxInt(current-1, po.config.MinParallelism)
			reason = "ä½ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«ã‚ˆã‚Šä¸¦åˆ—åº¦å‰Šæ¸›"
		}

	} else if int(activeTests) >= current && cpuUsage < po.config.CPUThresholdHigh {
		// å…¨ä¸¦åˆ—ã‚¹ãƒ­ãƒƒãƒˆãŒä½¿ç”¨ä¸­ã‹ã¤CPUã«ä½™è£• â†’ ä¸¦åˆ—åº¦å¢—åŠ 
		if current < po.config.MaxParallelism {
			optimal = minInt(current+1, po.config.MaxParallelism)
			reason = "é«˜åˆ©ç”¨ç‡ã«ã‚ˆã‚Šä¸¦åˆ—åº¦å¢—åŠ "
		}
	}

	return &OptimizationEvent{
		Timestamp:      time.Now(),
		OldParallelism: current,
		NewParallelism: optimal,
		Reason:         reason,
		CPUUsage:       cpuUsage,
		MemoryUsage:    memoryUsage,
		ActiveTests:    int(activeTests),
	}
}

// applyParallelismChange ä¸¦åˆ—åº¦å¤‰æ›´é©ç”¨
func (po *ParallelOptimizer) applyParallelismChange(event *OptimizationEvent) {
	po.config.CurrentParallelism = event.NewParallelism

	// Goã®ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¨­å®šæ›´æ–°
	runtime.GOMAXPROCS(event.NewParallelism)
}

// recordOptimizationEvent æœ€é©åŒ–ã‚¤ãƒ™ãƒ³ãƒˆè¨˜éŒ²
func (po *ParallelOptimizer) recordOptimizationEvent(event *OptimizationEvent) {
	po.optimizationHistory = append(po.optimizationHistory, event)

	// å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆæœ€æ–°100ä»¶ï¼‰
	if len(po.optimizationHistory) > 100 {
		po.optimizationHistory = po.optimizationHistory[1:]
	}
}

// logOptimizationEvent æœ€é©åŒ–ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°å‡ºåŠ›
func (po *ParallelOptimizer) logOptimizationEvent(event *OptimizationEvent) {
	log.Printf("âš¡ ä¸¦åˆ—åº¦æœ€é©åŒ–å®Ÿè¡Œ:")
	log.Printf("   ä¸¦åˆ—åº¦: %d â†’ %d", event.OldParallelism, event.NewParallelism)
	log.Printf("   ç†ç”±: %s", event.Reason)
	log.Printf("   CPUä½¿ç”¨ç‡: %.1f%%", event.CPUUsage*100)
	log.Printf("   ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: %.1f%%", event.MemoryUsage*100)
	log.Printf("   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆ: %d", event.ActiveTests)
}

// StartTest ãƒ†ã‚¹ãƒˆé–‹å§‹ç™»éŒ²
func (po *ParallelOptimizer) StartTest(testName string, priority int, resourceWeight float64) {
	po.testTracker.mu.Lock()
	defer po.testTracker.mu.Unlock()

	now := time.Now()
	po.testTracker.activeTests[testName] = &TestExecution{
		TestName:       testName,
		StartTime:      now,
		ResourceWeight: resourceWeight,
		Priority:       priority,
	}
	po.testTracker.testStartTime[testName] = now

	atomic.AddInt32(&po.metrics.ActiveTests, 1)
}

// FinishTest ãƒ†ã‚¹ãƒˆçµ‚äº†ç™»éŒ²
func (po *ParallelOptimizer) FinishTest(testName string, success bool) {
	po.testTracker.mu.Lock()
	defer po.testTracker.mu.Unlock()

	if startTime, exists := po.testTracker.testStartTime[testName]; exists {
		duration := time.Since(startTime)
		po.metrics.TotalTestDuration += duration

		delete(po.testTracker.activeTests, testName)
		delete(po.testTracker.testStartTime, testName)
	}

	atomic.AddInt32(&po.metrics.ActiveTests, -1)
	atomic.AddInt32(&po.metrics.CompletedTests, 1)

	if !success {
		atomic.AddInt32(&po.metrics.FailedTests, 1)
	}
}

// StartAutoOptimization è‡ªå‹•æœ€é©åŒ–é–‹å§‹
func (po *ParallelOptimizer) StartAutoOptimization(ctx context.Context) {
	if !po.config.AutoOptimize {
		return
	}

	ticker := time.NewTicker(po.config.MonitoringInterval)
	defer ticker.Stop()

	log.Printf("ğŸ¤– ä¸¦åˆ—åº¦è‡ªå‹•æœ€é©åŒ–é–‹å§‹ (é–“éš”: %v)", po.config.MonitoringInterval)

	for {
		select {
		case <-ctx.Done():
			log.Printf("ğŸ›‘ ä¸¦åˆ—åº¦è‡ªå‹•æœ€é©åŒ–åœæ­¢")
			return
		case <-ticker.C:
			event := po.OptimizeParallelism()
			_ = event // çµæœã‚’è¨˜éŒ²æ¸ˆã¿
		}
	}
}

// GetCurrentMetrics ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
func (po *ParallelOptimizer) GetCurrentMetrics() (*ParallelMetrics, *SystemResourceMonitor) {
	po.mu.RLock()
	defer po.mu.RUnlock()

	// ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
	po.updateResourceMonitor()
	po.updateMetrics()

	return po.metrics, po.resourceMonitor
}

// GetOptimizationHistory æœ€é©åŒ–å±¥æ­´å–å¾—
func (po *ParallelOptimizer) GetOptimizationHistory() []*OptimizationEvent {
	po.mu.RLock()
	defer po.mu.RUnlock()

	// ã‚³ãƒ”ãƒ¼ã‚’è¿”ã™
	history := make([]*OptimizationEvent, len(po.optimizationHistory))
	copy(history, po.optimizationHistory)
	return history
}

// GetCurrentParallelism ç¾åœ¨ã®ä¸¦åˆ—åº¦å–å¾—
func (po *ParallelOptimizer) GetCurrentParallelism() int {
	po.mu.RLock()
	defer po.mu.RUnlock()

	return po.config.CurrentParallelism
}

// ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
func minInt(a, b int) int {
	if a < b {
		return a
	}
	return b
}

func maxInt(a, b int) int {
	if a > b {
		return a
	}
	return b
}
